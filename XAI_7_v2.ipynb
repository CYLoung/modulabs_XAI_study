{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi4eZgYjY_03",
        "outputId": "99b0e774-2b81-4219-d27c-48c82dc99eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\chaey\\AppData\\Local\\Temp\\ipykernel_42876\\3771312607.py:2: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "k9ay4CZ2030g",
        "outputId": "d19a0e7b-db56-42d6-994c-1f560a7bf252"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "print(x_train.shape, x_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_test = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(5, (5, 5), activation='relu', input_shape=(28, 28, 1), padding = 'same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(5, (5, 5), activation='relu', padding = \"same\"),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(20, (5, 5), activation='relu', padding = 'same'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "# padding \n",
        "# same : input == output size\n",
        "# vaild : no padding\n",
        "# https://itstory1592.tistory.com/23\n",
        "# Build CNN model => 입력 이미지 크기가 작다?\n",
        "# https://stackoverflow.com/questions/71378093/valueerror-negative-dimension-size-caused-by-subtracting-2-from-1-for-node-c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 5)         130       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 5)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 5)         630       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 5)           0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 20)          2520      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 7, 7, 20)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 980)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                9810      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13090 (51.13 KB)\n",
            "Trainable params: 13090 (51.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_test.summary()\n",
        "# 7x7x20 980x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(5, (5, 5), activation='relu', input_shape=(28, 28, 1), padding = 'same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(5, (5, 5), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(20, (5, 5), activation='relu', padding = 'same'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "# same 을 안 넣으면 자동으로 padding을 넣어서 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9086"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.2893 - acc: 0.9092 - val_loss: 0.0790 - val_acc: 0.9742\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.1014 - acc: 0.9687 - val_loss: 0.0475 - val_acc: 0.9837\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0784 - acc: 0.9758 - val_loss: 0.0395 - val_acc: 0.9858\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.0651 - acc: 0.9791 - val_loss: 0.0418 - val_acc: 0.9863\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0572 - acc: 0.9819 - val_loss: 0.0306 - val_acc: 0.9898\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0528 - acc: 0.9831 - val_loss: 0.0335 - val_acc: 0.9902\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 75us/sample - loss: 0.0490 - acc: 0.9843 - val_loss: 0.0294 - val_acc: 0.9902\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 75us/sample - loss: 0.0465 - acc: 0.9847 - val_loss: 0.0313 - val_acc: 0.9910\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0446 - acc: 0.9858 - val_loss: 0.0304 - val_acc: 0.9903\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0424 - acc: 0.9863 - val_loss: 0.0310 - val_acc: 0.9906\n",
            "Test accuracy: 0.9906\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=50, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 계층별 활성화 함수 결과를 출력하는 코드 : getActivations\n",
        "2. LRP를 수행할 이미지 하나를 부르기\n",
        "3. 합성곱 신경망의 은닉 계층마다 활성화 함수를 구하는 코드\n",
        "4. 합성곱 신경망에 이미지를 입력하고 out_y 예측 결과를 구하는 코드\n",
        "5. 학습된 신경망으로부터 분류 가능성이 최대가 되는 카테고리를 구하는 코드\n",
        "6. 완전 연결 신경망에서 역전파 기울기를 구하는 수도 코드 : getGradient \n",
        "7. f_x로부터 바로 직전 은닉층의 타당성 전파 값을 구하는 코드\n",
        "8. FC 연결에서 LRP를 수행하는 코드. 예제 7.8의 역전파 기울기를 구하는 코드에 타당성 변수(relevance)를 곱한다 : backprop_dense\n",
        "9. 예제 7.9에서 만든 LRP 공식으로 𝑅3를 구하는 코드\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fRXWOTzu8M-o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2a403cfa430>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 예제 7.4 LRP를 수행할 이미지 하나를 부르는 과정\n",
        "\n",
        "# show test image\n",
        "idx = 4\n",
        "imageToUse = x_test[idx]\n",
        "plt.imshow(np.reshape(imageToUse,[28,28]), interpolation=\"nearest\", cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CG8d_4Wn8e-e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 모델에 정의된 층들 가져오기\n",
        "layers = model.layers\n",
        "\n",
        "# 각 층에서 가중치와 편향을 추출하여 리스트에 저장\n",
        "weights = []\n",
        "biases = []\n",
        "for layer in layers:\n",
        "    # 가중치와 편향 추출\n",
        "    if layer.weights:\n",
        "        layer_weights = layer.weights[0]  # 가중치\n",
        "        layer_biases = layer.weights[1]   # 편향\n",
        "        weights.append(layer_weights)\n",
        "        biases.append(layer_biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WJMp2Xbf8qXh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0] 0.00%\n",
            "[1] 0.00%\n",
            "[2] 0.00%\n",
            "[3] 0.00%\n",
            "[4] 100.00%\n",
            "[5] 0.00%\n",
            "[6] 0.00%\n",
            "[7] 0.00%\n",
            "[8] 0.00%\n",
            "[9] 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 이미지를 모델에 입력하여 예측 결과를 구하기\n",
        "predict = model.predict(imageToUse.reshape(1, 28, 28, 1))\n",
        "\n",
        "# 예측 결과 출력\n",
        "for idx, prob in enumerate(predict[0]):\n",
        "    print('[{}] {:.2%}'.format(idx, prob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2YVeTWpB81cC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "가장 높은 확률: 0.9999999\n"
          ]
        }
      ],
      "source": [
        "# 예제 7.7 학습된 합성곱 신경망으로부터 분류 가능성이 최대가 되는 카테고리를 구하는 코드\n",
        "\n",
        "# 예측된 확률 중 가장 높은 값을 구합니다.\n",
        "max_probability = np.max(predict)\n",
        "\n",
        "print(\"가장 높은 확률:\", max_probability)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SmnAqzvT9Aqy"
      },
      "outputs": [],
      "source": [
        "# 예제 7.8 완전 연결 신경망에서 역전파 기울기를 구하는 수도 코드\n",
        "\n",
        "# get FC layer gradient\n",
        "\n",
        "def getGradient(activation, weight, bias):\n",
        "    # Forward pass\n",
        "    W = tf.maximum(0., weight)\n",
        "    b = tf.maximum(0., bias)\n",
        "    z = tf.matmul(activation, W) + b\n",
        "\n",
        "    # Backward pass\n",
        "    dX = tf.matmul(1 / z, tf.transpose(W))\n",
        "    return dX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SpKTnlpwLWaK"
      },
      "outputs": [],
      "source": [
        "def backprop_dense(activation, weight, bias, relevance):\n",
        "    # Forward pass\n",
        "    z = tf.matmul(activation, weight) + bias\n",
        "    z_p = tf.maximum(0.0, z) + 1e-9  # Add a small constant to prevent division by zero\n",
        "\n",
        "    # Backward pass\n",
        "    s = relevance / z_p\n",
        "    c = tf.matmul(s, tf.transpose(weight))\n",
        "    return activation * c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_activations(layer, image):\n",
        "    units = layer(image.reshape(1, 28, 28, 1))\n",
        "    return units\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Shape must be rank 4 but is rank 2 for '{{node conv2d_3_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Const, conv2d_3_1/Conv2D/ReadVariableOp)' with input shapes: [1,784], [5,5,1,5].",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 각 레이어에서 활성화 값을 가져와서 activations에 추가\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# 입력 이미지에 대해 레이어의 활성화 값을 계산\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageToUse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(activation)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 계층별 활성화 함수 결과 출력\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\engine\\base_layer_v1.py:898\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    894\u001b[0m cast_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m    897\u001b[0m ):\n\u001b[1;32m--> 898\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(cast_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks)\n",
            "File \u001b[1;32mc:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:290\u001b[0m, in \u001b[0;36mConv.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compiled_convolution_op(\n\u001b[0;32m    287\u001b[0m         inputs, tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel)\n\u001b[0;32m    288\u001b[0m     )\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    293\u001b[0m     output_rank \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
            "File \u001b[1;32mc:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:262\u001b[0m, in \u001b[0;36mConv.convolution_op\u001b[1;34m(self, inputs, kernel)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     tf_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tf_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\chaey\\anaconda3\\envs\\py39_xai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1020\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1017\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1019\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_traceback:\n",
            "\u001b[1;31mValueError\u001b[0m: Shape must be rank 4 but is rank 2 for '{{node conv2d_3_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Const, conv2d_3_1/Conv2D/ReadVariableOp)' with input shapes: [1,784], [5,5,1,5]."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "imageToUse = imageToUse.reshape(-1, 784)\n",
        "\n",
        "# 모델의 레이어 리스트 가져오기\n",
        "layers = model.layers\n",
        "activations = []\n",
        "\n",
        "# 각 레이어에서 활성화 값을 가져와서 activations에 추가\n",
        "for layer in layers:\n",
        "    # 입력 이미지에 대해 레이어의 활성화 값을 계산\n",
        "    activation = layer(imageToUse)\n",
        "    activations.append(activation)\n",
        "\n",
        "# 계층별 활성화 함수 결과 출력\n",
        "for i, activation in enumerate(activations):\n",
        "    print(f\"Activation of layer {i + 1}: {activation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZZSsdYzK8TQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  예제 7.9 f_x로부터 바로 직전 은닉층의 타당성 전파 값을 구하는 코드\n",
        "R4 = predict\n",
        "R4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s0KYphKLmxP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 10)\n",
            "(980, 10)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'KerasTensor' object has no attribute 'reshape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(w\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 13\u001b[0m R3 \u001b[38;5;241m=\u001b[39m backprop_dense(\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m980\u001b[39m), w, b, R4)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(R3\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'reshape'"
          ]
        }
      ],
      "source": [
        "# 예제 7.10 예제 7.9에서 만든 LRP 공식으로 𝑅3를 구하는 코드\n",
        "\n",
        "# layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]\n",
        "# (1, 28, 28, 5) (1, 14, 14, 5) (1, 7, 7, 20)\n",
        "# activation, weights, biases\n",
        "a = activations.pop()\n",
        "w = weights.pop()\n",
        "b = biases.pop()\n",
        "\n",
        "print(a.shape)\n",
        "print(w.shape)\n",
        "\n",
        "R3 = backprop_dense(a.reshape(1,980), w, b, R4)\n",
        "\n",
        "print(R3.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbgIXdTkMFS1"
      },
      "outputs": [],
      "source": [
        "# 예제 7.11-(1) 언풀링 연산에서 LRP를 구하는 코드\n",
        "\n",
        "from tensorflow.python.ops import gen_nn_ops\n",
        "\n",
        "def backprop_pooling(activation, relevance):\n",
        "    # kernel size, strides\n",
        "    # if z is zero\n",
        "    ksize = strides = [1, 2, 2, 1]\n",
        "    z = tf.nn.max_pool(activation, ksize, strides, padding='SAME') + 1e-10\n",
        "    s = relevance / z\n",
        "    # input, argmax, argmax_mask\n",
        "    c = gen_nn_ops.max_pool_grad(activation, z, s, ksize, strides, padding='SAME')\n",
        "    return activation * c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FeiGacjMlZ3"
      },
      "outputs": [],
      "source": [
        "# 예제 7.11-(2) 역합성곱 연산에서 LRP를 구하는 코드\n",
        "\n",
        "def backprop_conv(activation, weight, bias, relevance):\n",
        "    strides = [1, 1, 1, 1]\n",
        "    w = tf.maximum(0., weight)\n",
        "    b = tf.maximum(0., bias)\n",
        "    z = tf.nn.conv2d(activation, w, strides, padding='SAME')\n",
        "    z = tf.nn.bias_add(z, b)\n",
        "    s = relevance / z\n",
        "    c = tf.nn.conv2d_backprop_input(tf.shape(activation), w, s, strides, padding='SAME')\n",
        "    return activation * c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxxvCniVNEMe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 7, 7, 5)\n",
            "WARNING:tensorflow:From C:\\Users\\chaey\\AppData\\Local\\Temp\\ipykernel_42532\\1712813241.py:10: The name tf.nn.conv2d_backprop_input is deprecated. Please use tf.nn.conv2d_transpose instead.\n",
            "\n",
            "(1, 7, 7, 5)\n",
            "WARNING:tensorflow:From C:\\Users\\chaey\\AppData\\Local\\Temp\\ipykernel_42532\\1142312502.py:9: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "(1, 14, 14, 5)\n"
          ]
        }
      ],
      "source": [
        "# 예제 7.12 𝑅3 벡터로부터 역합성곱과 언풀링 연산을 수행하고 𝑅2 벡터를 구하는 코드\n",
        "\n",
        "# layers = [hidden_1, pool_1, hidden_2, pool_2]\n",
        "# (1, 28, 28, 5)(1, 14, 14, 5)(1, 7, 7, 20)\n",
        "# activation, weights, biases\n",
        "w = weights.pop()\n",
        "b = biases.pop()\n",
        "p = activations.pop()\n",
        "a = activations.pop()\n",
        "print(p.shape)\n",
        "\n",
        "# convolution backprop\n",
        "R_conv = backprop_conv(p, w, b, tf.reshape(R3, [1, 7, 7, 20]))\n",
        "print(R_conv.shape)\n",
        "R2 = backprop_pooling(a, R_conv)\n",
        "print(R2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQP2Oz54NSA7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 14, 14, 5)\n",
            "(1, 28, 28, 5)\n",
            "0.97247183\n"
          ]
        }
      ],
      "source": [
        "# 예제 7.11 𝑅2에서 역합성곱과 언풀링 과정을 수행하고 𝑅1 벡터를 구하는 코드\n",
        "\n",
        "# layers = [hidden_1, pool_1]\n",
        "# (1, 28, 28, 5)(1, 14, 14, 5)\n",
        "# activation, weights, biases\n",
        "w = weights.pop()\n",
        "b = biases.pop()\n",
        "p = activations.pop()\n",
        "a = activations.pop()\n",
        "\n",
        "# convolution backprop\n",
        "R_conv = backprop_conv(p, w, b, R2)\n",
        "print(R_conv.shape)\n",
        "\n",
        "R1 = backprop_pooling(a, R_conv)\n",
        "print(R1.shape)\n",
        "print(np.sum(sess.run(R1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ua8Owq1R70l"
      },
      "outputs": [],
      "source": [
        "# 예제 7.12 𝑅1 결과에서 원본 이미지까지 LRP를 수행하는 코드\n",
        "\n",
        "img_activations = getActivations(x_image, imageToUse)\n",
        "w = weights.pop()\n",
        "b = biases.pop()\n",
        "R0 = backprop_conv(img_activations, w, b, R1)\n",
        "LRP_out = sess.run(R0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dzrUIXzR9Gt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x206a26bb288>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 예제 7.13 원본 이미지 형태로 타당성 전파를 수행하고 결과물을 이미지 형태로 출력하는 코드\n",
        "\n",
        "plt.imshow(LRP_out.reshape(28, 28), interpolation=\"nearest\", cmap=plt.cm.jet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T11R72fYStmo"
      },
      "outputs": [],
      "source": [
        "# 예제 7.13 합성곱 신경망 전체에 대해 LRP를 수행하는 코드\n",
        "\n",
        "def getLRP(img):\n",
        "    predict = sess.run(out_y, feed_dict={x: img.reshape([-1, 784]), keep_prob:1.0})[0]\n",
        "    layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]\n",
        "    weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*weights.*')\n",
        "    biases = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*biases.*')\n",
        "\n",
        "    # layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]\n",
        "    activations = []\n",
        "    for layer in layers:\n",
        "        activations.append(getActivations(layer, img))\n",
        "\n",
        "    # get f_x\n",
        "    f_x = max(predict)\n",
        "\n",
        "    # get R4\n",
        "    predict[predict < 0] = 0\n",
        "    R4 = predict\n",
        "\n",
        "    # get R3\n",
        "    a = activations.pop()\n",
        "    w = weights.pop()\n",
        "    b = biases.pop()\n",
        "    R3 = backprop_dense(a.reshape(1,980), w, b, R4)\n",
        "\n",
        "    # get R2\n",
        "    w = weights.pop()\n",
        "    b = biases.pop()\n",
        "    p = activations.pop()\n",
        "    a = activations.pop()\n",
        "    R_conv = backprop_conv(p, w, b, tf.reshape(R3, [1, 7, 7, 20]))\n",
        "    R2 = backprop_pooling(a, R_conv)\n",
        "\n",
        "    # get R1\n",
        "    w = weights.pop()\n",
        "    b = biases.pop()\n",
        "    p = activations.pop()\n",
        "    a = activations.pop()\n",
        "    R_conv = backprop_conv(p, w, b, R2)\n",
        "    R1 = backprop_pooling(a, R_conv)\n",
        "\n",
        "    # get R0\n",
        "    img_activations = getActivations(x_image, img)\n",
        "    w = weights.pop()\n",
        "    b = biases.pop()\n",
        "    R0 = backprop_conv(img_activations, w, b, R1)\n",
        "    LRP_out = sess.run(R0)\n",
        "    return LRP_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDlFtltTSuY9"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'keras.api._v2.keras.datasets.mnist' has no attribute 'test'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     17\u001b[0m     img_idx \u001b[38;5;241m=\u001b[39m mnist_dict[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mmnist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[38;5;241m.\u001b[39mimages[img_idx]\n\u001b[0;32m     19\u001b[0m     lrp \u001b[38;5;241m=\u001b[39m getLRP(img)\n\u001b[0;32m     20\u001b[0m     nums\u001b[38;5;241m.\u001b[39mappend(lrp)\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.datasets.mnist' has no attribute 'test'"
          ]
        }
      ],
      "source": [
        "# 예제 7.13 합성곱 신경망 전체에 대해 LRP를 수행하는 코드\n",
        "\n",
        "# get MNIST dataset index dict\n",
        "mnist_dict = {}\n",
        "idx = 0\n",
        "for i in y_test:\n",
        "    label = np.where(i == np.amax(i))[0][0]\n",
        "    if mnist_dict.get(label):\n",
        "        mnist_dict[label].append(idx)\n",
        "    else:\n",
        "        mnist_dict[label] = [idx]\n",
        "    idx += 1\n",
        "\n",
        "# get LRP\n",
        "nums = []\n",
        "for i in range(10):\n",
        "    img_idx = mnist_dict[i][0]\n",
        "    img = mnist.test.images[img_idx]\n",
        "    lrp = getLRP(img)\n",
        "    nums.append(lrp)\n",
        "\n",
        "# plot images\n",
        "plt.figure(figsize=(20,10))\n",
        "for i in range(2):\n",
        "    for j in range(5):\n",
        "        idx = 5 * i + j\n",
        "        plt.subplot(2, 5, idx + 1)\n",
        "        plt.title('digit: {}'.format(idx))\n",
        "        plt.imshow(nums[idx].reshape([28, 28]), cmap=plt.cm.jet)\n",
        "        plt.colorbar(orientation='horizontal')\n",
        "plt.tight_layout()\n",
        "sess.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.raw_ops # 아주 밑에 단까지 가는!!.."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
